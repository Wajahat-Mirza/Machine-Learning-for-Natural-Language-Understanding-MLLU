{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW-1 Part 1. Spam Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg_SZXTBo2et"
      },
      "source": [
        "# Muhammad Wajahat Mirza\n",
        "\n",
        "## NetID: mwm356\n",
        "\n",
        "## MLLU Homework 01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Z8eeN5IW9q"
      },
      "source": [
        "# Part 1.\n",
        "\n",
        "The deadline for Part 1 is **2 pm Feb 11, 2021**.   \n",
        "You should submit a `.ipynb` file with your solutions to NYU Classes.\n",
        "\n",
        "---\n",
        "Spam filtering is a well-studied NLP classification problem that is used in many commercial products nowadays, including email clients and mobile SMS apps.\n",
        "\n",
        "In this assignment we will train a logistic regression model to classify each text in the SMS Spam Collection dataset as either spam or legitimate. This dataset consists of 5,574 English SMS messages, each tagged as either spam or ham (legitimate). The column 'v1' contains the label and the column 'v2' contains the SMS text. We will pre-process and convert the texts into bag-of-words features for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZd0LJzbISPd"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "First, we download the SMS Spam Collection Dataset. The below command downloads the dataset from [Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset/data#) and loads it to [Google Drive](https://drive.google.com/open?id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvGErs2oHkWU",
        "outputId": "b1b95de3-802f-453b-a799-44c06708623f"
      },
      "source": [
        "!wget 'https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR' -O spam.csv "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-11 15:09:29--  https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR\n",
            "Resolving docs.google.com (docs.google.com)... 64.233.189.101, 64.233.189.100, 64.233.189.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|64.233.189.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ceklobioragack96ad6spr8833aj0jr7/1613056125000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-11 15:09:30--  https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ceklobioragack96ad6spr8833aj0jr7/1613056125000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download\n",
            "Resolving doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)... 108.177.125.132, 2404:6800:4008:c01::84\n",
            "Connecting to doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 503663 (492K) [text/csv]\n",
            "Saving to: ‘spam.csv’\n",
            "\n",
            "spam.csv            100%[===================>] 491.86K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2021-02-11 15:09:30 (97.1 MB/s) - ‘spam.csv’ saved [503663/503663]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcHV1lUwtH-n",
        "outputId": "7c33b202-bcb8-4298-e26d-5700fafe5569"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spam.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1h66TFEo2ez"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JCpWT-8o2e0"
      },
      "source": [
        "### This is to download NLTK packages. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeRN05Odo2e0",
        "outputId": "bb511fd5-c18e-412f-98fc-d2529a23d347"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "import ssl\n",
        "\n",
        "'''If you are using jupyter notebook, pop up screen will ask to download corpuses. \n",
        "   Once download is complete, close the pop up window to proceed'''\n",
        "'''If you are using google Colab, just download first iteration of packages.\n",
        "    Upon second action input bar, type q'''\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cOutORro2e1",
        "outputId": "e7c2505c-9a56-409f-b5c8-5421043d86ac"
      },
      "source": [
        "!pip install cleanco\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from cleanco import prepare_terms, basename\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cleanco\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/23/4fa92dae854e36ed00f006c74b070aacb1989aa0b5570d3364ab13569d5a/cleanco-2.1-py3-none-any.whl\n",
            "Installing collected packages: cleanco\n",
            "Successfully installed cleanco-2.1\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOGG0N_vo2e1"
      },
      "source": [
        "## Data Import "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXVQCF-ovo4G"
      },
      "source": [
        "Now we preview the data. There are two columns: `v1` -- the label, which indicates whether the text is spam or ham (legitimate), and `v2` -- the text of the message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "BiKE89v0zMiY",
        "outputId": "1af55fb3-7ad8-44ff-d74c-6a1ddf94e4ae"
      },
      "source": [
        "\n",
        "'''Read the csv'''\n",
        "df = pd.read_csv(\"spam.csv\", usecols=[\"v1\", \"v2\"], encoding='latin-1')\n",
        "# 1 - spam, 0 - ham\n",
        "df.v1 = (df.v1 == \"spam\").astype(\"int\")\n",
        "print(\"Data Size: \", df.shape[0])\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Size:  5572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   v1                                                 v2\n",
              "0   0  Go until jurong point, crazy.. Available only ...\n",
              "1   0                      Ok lar... Joking wif u oni...\n",
              "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   0  U dun say so early hor... U c already then say...\n",
              "4   0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlCf2SaGo2e2"
      },
      "source": [
        "## Split the data using pandas and numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXQhTzrCv-Nk"
      },
      "source": [
        "Your task is to randomly split the data into training, validation, and test sets. Make sure that each row appears in only one of the splits and that training contains 70% of the data, validation contains 15%, and test contains 15%. You can compute the size of each split afterwards to sanity check your code. **You may use numpy and pandas functions, but please do not use sklearn.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeTvXhvpo2e2",
        "outputId": "b345637c-6dcb-4a01-9823-2869e2e7e386"
      },
      "source": [
        "# 0.15 for val, 0.15 for test, 0.7 for train\n",
        "\n",
        "val_size = int(df.shape[0] * 0.15)\n",
        "test_size = int(df.shape[0] * 0.15)\n",
        "train_size = int(df.shape[0] * 0.70)\n",
        "\n",
        "data = df.sample(frac=1, random_state=1)\n",
        "print(\"\\nSize of data without random sampling: {}\\nSize of data with  random  sampling : {}\\n\"\\\n",
        "      .format(data.shape[0], df.shape[0]))\n",
        "\n",
        "count_ham = 0\n",
        "count_spam = 0\n",
        "for i in range(data.shape[0]):\n",
        "    if (data[\"v1\"][i] == 0):\n",
        "        count_ham += 1\n",
        "    else:\n",
        "        count_spam += 1\n",
        "\n",
        "print(\"Data has total: \\n{} HAM entries\\n{} SPAM entries.\\n\\n{:.3}% of data is SPAM and {:.3}% is HAM\\n\"\\\n",
        "      .format(count_ham, count_spam, (count_spam/data.shape[0])*100, (count_ham/data.shape[0])*100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Size of data without random sampling: 5572\n",
            "Size of data with  random  sampling : 5572\n",
            "\n",
            "Data has total: \n",
            "4825 HAM entries\n",
            "747 SPAM entries.\n",
            "\n",
            "13.4% of data is SPAM and 86.6% is HAM\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsDfaIN6o2e3"
      },
      "source": [
        "### Verify Random Split and Random State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVMxT9nXo2e3",
        "outputId": "9d557727-6255-418f-cf02-833a5de485de"
      },
      "source": [
        "'''For testing purposes to verify that data split is random'''\n",
        "'''Use Random State seed == 1 to make sure random split is same at each re-run '''\n",
        "'''Reminder: 1 - spam, 0 - ham'''\n",
        "print(\"\\n=================Random Split with Seeding========================\")\n",
        "for i in range(2):\n",
        "    data_test = df.sample(frac=1, random_state=None)\n",
        "    print(\"\\n\",data_test.head(),\"\\n\")\n",
        "print(\"=================Seeding Random Split========================\")\n",
        "for i in range(2):\n",
        "    data_test = df.sample(frac=1, random_state=1)\n",
        "    print(\"\\n\",data_test.head(),\"\\n\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=================Random Split with Seeding========================\n",
            "\n",
            "       v1                                                 v2\n",
            "2522   0  Dunno lei... I might b eatin wif my frens... I...\n",
            "3727   0  The search 4 happiness is 1 of d main sources ...\n",
            "2677   0  Where in abj are you serving. Are you staying ...\n",
            "4402   0                             Just getting back home\n",
            "2462   0  Rose needs water, season needs change, poet ne... \n",
            "\n",
            "\n",
            "       v1                                                 v2\n",
            "1251   1  Please CALL 08712402779 immediately as there i...\n",
            "403    0   None of that's happening til you get here though\n",
            "585    0  Tell them u have a headache and just want to u...\n",
            "3344   0      Reverse is cheating. That is not mathematics.\n",
            "2718   1  18 days to Euro2004 kickoff! U will be kept in... \n",
            "\n",
            "=================Seeding Random Split========================\n",
            "\n",
            "       v1                                                 v2\n",
            "1078   0                           Convey my regards to him\n",
            "4028   0           [Û_] anyway, many good evenings to u! s\n",
            "958    0  My sort code is  and acc no is . The bank is n...\n",
            "4642   0                        Sorry i din lock my keypad.\n",
            "4674   1  Hi babe its Chloe, how r u? I was smashed on s... \n",
            "\n",
            "\n",
            "       v1                                                 v2\n",
            "1078   0                           Convey my regards to him\n",
            "4028   0           [Û_] anyway, many good evenings to u! s\n",
            "958    0  My sort code is  and acc no is . The bank is n...\n",
            "4642   0                        Sorry i din lock my keypad.\n",
            "4674   1  Hi babe its Chloe, how r u? I was smashed on s... \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-TMt1uho2e3"
      },
      "source": [
        "### Verifying Data Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga5Qydpw-gdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4766048-7180-45a6-98d2-45e6673d110e"
      },
      "source": [
        "\n",
        "train_set = data[:train_size]\n",
        "test_set = data[train_size:test_size + train_size+1]\n",
        "val_set = data[test_size + train_size + 1:]\n",
        "\n",
        "print(\"Size of train data: {}\\nSize of test data: {}\\nSize of validate data: {}\"\\\n",
        "      .format(train_set.shape[0],\\\n",
        "              test_set.shape[0],\\\n",
        "              val_set.shape[0]))\n",
        "print(\"\\nVerification of Data Split:\\nPercentage breakdown\\tTrain    = {:.3}%\\\n",
        "    \\n\\t\\t\\tTest     = {:.3}%\\\n",
        "    \\n\\t\\t\\tValidate = {:.3}%\"\\\n",
        "      .format((train_set.shape[0]/df.shape[0])*100,\\\n",
        "               (test_set.shape[0]/df.shape[0])*100,\\\n",
        "               (val_set.shape[0]/df.shape[0])*100))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of train data: 3900\n",
            "Size of test data: 836\n",
            "Size of validate data: 836\n",
            "\n",
            "Verification of Data Split:\n",
            "Percentage breakdown\tTrain    = 70.0%    \n",
            "\t\t\tTest     = 15.0%    \n",
            "\t\t\tValidate = 15.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUtp5W30o2e4"
      },
      "source": [
        "### Assigning Text Data and Label as per the split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR2mgWm7o2e4",
        "outputId": "f7f828ac-3bdf-4897-a7d5-2858e36beb97"
      },
      "source": [
        "'''Now, assign the values of text and label to each of the three split datasets'''\n",
        "\n",
        "train_texts, train_labels = train_set[\"v2\"], train_set[\"v1\"]\n",
        "val_texts, val_labels     = test_set[\"v2\"], test_set[\"v1\"]\n",
        "test_texts, test_labels   = val_set[\"v2\"], val_set[\"v1\"]\n",
        "\n",
        "print(\"\\nData assignment verification (text size, label size):\\n\")\n",
        "print(\"Train: \",train_texts.shape[0], train_labels.shape[0])\n",
        "print(\"Test : \",val_texts.shape[0], val_labels.shape[0])\n",
        "print(\"Validate: \",test_texts.shape[0], test_labels.shape[0])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Data assignment verification (text size, label size):\n",
            "\n",
            "Train:  3900 3900\n",
            "Test :  836 836\n",
            "Validate:  836 836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGyHG4lBISP2"
      },
      "source": [
        "## Data Processing\n",
        "\n",
        "The task is to create bag-of-words features: tokenize the text, index each token, represent the sentence as a dictionary of tokens and their counts, limit the vocabulary to $n$ most frequent tokens. In Lab 2 we will use the built-in `sklearn` function, `sklearn.feature_extraction.text.CountVectorizer`. \n",
        "**In this HW, you are required to implement the class `Vectorizer` on your own without using `sklearn` built-in functions.**\n",
        "\n",
        "Function `preprocess_data` takes the list of texts and returns list of (lists of tokens). \n",
        "You may use [spacy](https://spacy.io/) or [nltk](https://www.nltk.org/) text processing libraries in `preprocess_data` function. \n",
        "\n",
        "Class `Vectorizer` is used to vectorize the text and to create a matrix of features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVlWfv3Qo2e4"
      },
      "source": [
        "$\\textbf{Process:}$\n",
        "\n",
        "                    Punctuation were kept as they are indicative of SPAM\n",
        "                    Tokens are all lowerCase\n",
        "                    Stopwords are removed to adjust the Data Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "793EFaQYhHeR",
        "scrolled": false
      },
      "source": [
        "\n",
        "def remove_stopwords(tokenized_data):\n",
        "    \n",
        "    newText = []\n",
        "    for word in tokenized_data:\n",
        "        if word not in stopwords.words('english'):\n",
        "            newText.append(word)\n",
        "            \n",
        "    return newText\n",
        "\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # This function should return a list of lists of pre-processed tokens, where\n",
        "    # each nested list contains the tokens for a single message. To pre-process\n",
        "    # each message, tokenize the message and lowercase all text.\n",
        " \n",
        "    tokenized_data = []\n",
        "    for mesg in data:\n",
        "        mesg = mesg.lower()\n",
        "        \n",
        "        tokenized_data.append(nltk.word_tokenize(mesg))\n",
        "    \n",
        "    \n",
        "    ''' Removing stopwords from list of tokenized words'''\n",
        "    preprocessed_data = []\n",
        "    for i in range(len(tokenized_data)):\n",
        "        without_stopwords = remove_stopwords(tokenized_data[i])\n",
        "        preprocessed_data.append(without_stopwords)\n",
        "    print(\"Verfiy the original length {} of data with preprocessed data: {}\".format(len(data), len(preprocessed_data)))\n",
        "    \n",
        "    return preprocessed_data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSMpk6TNo2e5",
        "outputId": "7ae04863-e6cc-4875-a804-682a241b2d16"
      },
      "source": [
        "print(\"\\nPreprocessing Training Data:\")\n",
        "train_data = preprocess_data(train_texts)\n",
        "print(\"\\nPreprocessing Validating Data:\")\n",
        "val_data = preprocess_data(val_texts)\n",
        "print(\"\\nPreprocessing Test Data:\")\n",
        "test_data = preprocess_data(test_texts)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preprocessing Training Data:\n",
            "Verfiy the original length 3900 of data with preprocessed data: 3900\n",
            "\n",
            "Preprocessing Validating Data:\n",
            "Verfiy the original length 836 of data with preprocessed data: 836\n",
            "\n",
            "Preprocessing Test Data:\n",
            "Verfiy the original length 836 of data with preprocessed data: 836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGLg6udky1zo"
      },
      "source": [
        "### You can add more features to the feature matrix. \n",
        "\n",
        "(Not required, but worth extra credit points.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdKS3TRTo2e6"
      },
      "source": [
        "### Functions for Additional Features\n",
        "\n",
        "                Additional Features include:\n",
        "                        Detecting a link in a Token\n",
        "                        Checking for a Phone num\n",
        "                        Seeing Company Name, indicative of SPAM in most cases\n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POj4DmgQo2e6"
      },
      "source": [
        "def linkDetector(string): \n",
        "   \n",
        "    return len([x[0] for x in \\\n",
        "                re.findall(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|\"\n",
        "                           \"[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\"\n",
        "                           \"\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\"\"\n",
        "                           \".,<>?«»“”‘’]))\",\n",
        "                     string)] )\n",
        "\n",
        "\n",
        "def company(business_name):\n",
        "    terms = prepare_terms()\n",
        "    name = basename(business_name, terms, prefix=True, middle=True, suffix=False)\n",
        "    x = name.split(\",\")\n",
        "    if len(x) <= 1:\n",
        "        return False\n",
        "    else:\n",
        "        for term in terms:\n",
        "            if x[1] in term[1]:\n",
        "                print(x[1])\n",
        "                return True\n",
        "            else:\n",
        "                return False"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM2qpOKpjVbD",
        "scrolled": true
      },
      "source": [
        "class Vectorizer():\n",
        "    def __init__(self, max_features):\n",
        "        self.max_features = max_features\n",
        "        self.vocab_list = {}\n",
        "        self.token_to_index = {}\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        # Given a list of lists of tokens, create a vocab list, self.vocab_list, \n",
        "        # using the most frequent \"max_features\" tokens. \n",
        "        # Also create a token indexer, self.token_to_index, that will return the index \n",
        "        # of the token in self.vocab_list.\n",
        "        \n",
        "        '''Adding tokens to vocab_list along with their frequency'''\n",
        "        for sentence in dataset:\n",
        "            for word in sentence:\n",
        "                if word not in self.vocab_list:\n",
        "                    self.vocab_list[word] = 1\n",
        "                else:\n",
        "                    self.vocab_list[word] += 1\n",
        "                    \n",
        "        '''converting the frequency work dictionary to a tuple'''\n",
        "        '''Sorting the list of tuples on freqency count'''\n",
        "        '''Take the top Max_features Tokens based on frequency'''\n",
        "        dictTotuple = [(word, count) for word, count in self.vocab_list.items()]  \n",
        "        sorted_by_freq = sorted(dictTotuple, key=lambda tup: tup[1]) \n",
        "        \n",
        "        \n",
        "        self.vocab_list = []\n",
        "\n",
        "        newlst = sorted_by_freq[-self.max_features:]\n",
        "        \n",
        "        \n",
        "        for element in newlst:\n",
        "            self.vocab_list.append(element[0])\n",
        "        \n",
        "        '''Creating the token_to_index for the top selected vocab'''\n",
        "        for i in range(len(self.vocab_list)):\n",
        "            self.token_to_index[self.vocab_list[i]] = i\n",
        "        \n",
        "        \n",
        "    def transform(self, dataset):\n",
        "        # This function transforms the text dataset (a list of lists of tokens) \n",
        "        # into a matrix, \"data_matrix,\" where the entry located at (i, j) is 1 \n",
        "        # if sample i of the dataset contains token j in the vocab list, \n",
        "        # and 0 otherwise.\n",
        "        \n",
        "        '''data_matrix = N * (max_features + d)'''\n",
        "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)+3))\n",
        "        print(\"Data_Matrix Shape (N * (max_features + d)): \", data_matrix.shape)\n",
        "    \n",
        "\n",
        "        for i in range(len(dataset)):\n",
        "            for j in range(len(dataset[i])):\n",
        "                \n",
        "                d = (dataset[i][j], 'utf-8')\n",
        "                \n",
        "                if (dataset[i][j] in self.vocab_list):\n",
        "                    data_matrix[i][j] += 1\n",
        "                    \n",
        "                else:\n",
        "                    data_matrix[i][j] = 0 \n",
        "                    \n",
        "                if (linkDetector(dataset[i][j]) > 0):\n",
        "                    data_matrix[i][-1] = 1\n",
        "                \n",
        "                if ((d[0].isnumeric() == True and len(d[0]) > 3)):\n",
        "                    data_matrix[i][-2] = 1\n",
        "                    \n",
        "                if (company(dataset[i][j]) == True):\n",
        "                    data_matrix[i][-3] = 1\n",
        "                    \n",
        "        return data_matrix\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITq8ZG-Co2e7"
      },
      "source": [
        "### Call the Vectorizer on each dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXMrZXlZjcH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e90bb42-d2c1-4fef-b54f-9c0da57b2990"
      },
      "source": [
        "\n",
        "max_features = 1000 # TODO: Replace None with a number\n",
        "vectorizer = Vectorizer(max_features=max_features)\n",
        "vectorizer.fit(train_data)\n",
        "\n",
        "print(\"\\nTransforming Training Data\")\n",
        "X_train = vectorizer.transform(train_data)\n",
        "print(\"\\nTransforming Validation Data\")\n",
        "X_val = vectorizer.transform(val_data)\n",
        "print(\"\\nTransforming Test Data\")\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "'''setting labels to arrays'''\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "vocab = vectorizer.vocab_list\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Transforming Training Data\n",
            "Data_Matrix Shape (N * (max_features + d)):  (3900, 1003)\n",
            "\n",
            "Transforming Validation Data\n",
            "Data_Matrix Shape (N * (max_features + d)):  (836, 1003)\n",
            "\n",
            "Transforming Test Data\n",
            "Data_Matrix Shape (N * (max_features + d)):  (836, 1003)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtm7a6JWu9-3"
      },
      "source": [
        "## Model\n",
        "\n",
        "We train a logistic regression model on the bag-of-words features and save predictions for the training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq9stSAbAIZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d293d2a9-d7b3-4195-8f24-07900c2381b0"
      },
      "source": [
        "\n",
        "\n",
        "# Define Logistic Regression model\n",
        "'''Using l2 regularization since our Solver is liblinear'''\n",
        "model = LogisticRegression(penalty='l2',random_state=0, solver='liblinear', max_iter=100, verbose=1)\n",
        "\n",
        "# Fit the model to training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction using the trained model\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_val_pred = model.predict(X_val)\n",
        "y_test_pred = model.predict(X_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j-Abw7JOqD_"
      },
      "source": [
        "## Performance of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akg9LvP5DGE8"
      },
      "source": [
        "Your task is to report train, val, test accuracies and F1 scores.\n",
        "**You are required to implement `accuracy_score` and `f1_score` methods without using built-in python functions.**\n",
        "\n",
        "Your model should achieve at least **0.95** test accuracy and **0.90** test F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWx20DmVo2e8"
      },
      "source": [
        "### Solution: Performance Model\n",
        "#### Accuracy, Precision, Recall, and F-1 Score\n",
        "\n",
        "A true positive is an outcome where the model correctly predicts the positive class. Similarly, a true negative is an outcome where the model correctly predicts the negative class.\n",
        "\n",
        "A false positive is an outcome where the model incorrectly predicts the positive class. And a false negative is an outcome where the model incorrectly predicts the negative class.\n",
        "\n",
        "In my performance model, \n",
        "            \n",
        "               if a message is correctly predicted HAM, it is True Positive.\n",
        "               if a message is correctly predicted SPAM, it is True Negative.\n",
        "               if a message is HAM but predicted as SPAM, it is False Negative.\n",
        "               if a message is SPAM but predicted as HAM, it is False Positive.\n",
        "\n",
        "\n",
        "$\\textbf{Key:}$\n",
        "\n",
        "                TP = True Positive\n",
        "                TN = True Negative\n",
        "                FN = False Negative\n",
        "                FP = False Positive\n",
        "\n",
        "$\\textbf{Formulas:}$\n",
        "\n",
        "$$Accuracy = \\cfrac{TP + TN}{TP + TN + FN + FP}$$\n",
        "\n",
        "$$Precision = \\cfrac{TP}{TP + FP}$$\n",
        "\n",
        "\n",
        "$$Recall = \\cfrac{TP}{TP + FN}$$\n",
        "\n",
        "$$F-1 Score = 2 * \\cfrac{precision * recall}{precision + recall}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVRdHNwqo2e8"
      },
      "source": [
        "def calculate_confusion_matrix(ground_truth, prediction, datatype):\n",
        "    '''\n",
        "    Each correct ham               == true pos\n",
        "    Each correct spam              == true neg\n",
        "    Ham incorrectly labeled spam   == false neg\n",
        "    Spam incorrectly labeled ham   == false pos\n",
        "    '''\n",
        "    # 1 - spam, 0 - ham\n",
        "    true_pos  = 0\n",
        "    true_neg  = 0\n",
        "    false_neg = 0\n",
        "    false_pos = 0\n",
        "    \n",
        "    for val, pred in zip(ground_truth,prediction):\n",
        "        if (val == 0 and pred == 0):\n",
        "            true_pos += 1\n",
        "        elif (val == 1 and pred == 1):\n",
        "            true_neg += 1\n",
        "        elif(val == 0 and pred == 1):\n",
        "            false_neg += 1\n",
        "        elif (val == 1 and pred == 0):\n",
        "            false_pos += 1\n",
        "        else:\n",
        "            print(\"An invalid value occured.\")\n",
        "            continue\n",
        "    \n",
        "    return true_pos, true_neg, false_neg, false_pos\n",
        "        \n",
        "def precision(true_pos, false_pos):\n",
        "    return (true_pos/(true_pos + false_pos))\n",
        "    \n",
        "def recall(true_pos, false_neg):\n",
        "    return (true_pos/(true_pos + false_neg))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chqVbKH6kZyY"
      },
      "source": [
        "\n",
        "def accuracyScore(y_true, y_pred, datatype): \n",
        "    # Calculate accuracy of the model's prediction\n",
        "\n",
        "    true_pos, true_neg, false_neg, false_pos = calculate_confusion_matrix(y_true, y_pred, datatype)\n",
        "    accuracy = (true_pos + true_neg)/(true_pos + true_neg + false_neg + false_pos )\n",
        "    return accuracy\n",
        "\n",
        "def f1Score(y_true, y_pred, datatype): \n",
        "    # Calculate F1 score of the model's prediction\n",
        "    \n",
        "    true_pos, true_neg, false_neg, false_pos = calculate_confusion_matrix(y_true, y_pred, datatype)\n",
        "    precis = precision(true_pos, false_pos)\n",
        "    recal = recall(true_pos, false_neg)\n",
        "    f1 = 2 * ((precis * recal)/(precis + recal))\n",
        "    return f1\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqrMw0udDD04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da8b895-990d-4bb3-c5a3-c1a7ff8787fb"
      },
      "source": [
        "print()\n",
        "print(\"\\nTraining accuracy: {:.3f}, F1 score: {:.3f}\\n\".format(accuracyScore(y_train, y_train_pred, \"Training\"), f1Score(y_train, y_train_pred, \"Training\")))\n",
        "print(\"\\nValidation accuracy: {:.3f}, \"\n",
        "      \"F1 score: {:.3f}\\n\".format(accuracyScore(y_val, y_val_pred, \"Validation\"), f1Score(y_val, y_val_pred, \"Validation\")))\n",
        "print(\"\\nTest accuracy: {:.3f}, \"\n",
        "      \"F1 score: {:.3f}\\n\".format(accuracyScore(y_test, y_test_pred, \"Test\"),f1Score(y_test, y_test_pred,\"Test\")))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training accuracy: 0.964, F1 score: 0.979\n",
            "\n",
            "\n",
            "Validation accuracy: 0.972, F1 score: 0.984\n",
            "\n",
            "\n",
            "Test accuracy: 0.971, F1 score: 0.984\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODZs1x-Xo2e9"
      },
      "source": [
        "### Verify Accuracy and F-1 Score using Sklearn\n",
        "\n",
        "            Sklearn F1 score is lower than what I got in my function. \n",
        "            It is due to the \"Average\" parameter that is being used. \n",
        "            If this parameter is changed to micro, F-1 Score increase. \n",
        "            If it is changed to \"macro\" it decreased. \n",
        "            Thus, \"weighted\" appears a good indicator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-AXK89zo2e9",
        "outputId": "5ed50fb3-b749-4434-a80c-bc06946197b1"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print()\n",
        "print(\"Accuracy Train: {:.3}\".format(accuracy_score(y_train, y_train_pred)))\n",
        "print(\"F1 score Train: {:.3}\".format(f1_score(y_train, y_train_pred, average='weighted')))\n",
        "print()\n",
        "\n",
        "print(\"Accuracy Test: {:.3}\".format(accuracy_score(y_test, y_test_pred)))\n",
        "print(\"F1 score Test: {:.3}\".format(f1_score(y_test, y_test_pred, average='weighted')))\n",
        "print()\n",
        "\n",
        "print(\"Accuracy Val: {:.3}\".format(accuracy_score(y_val, y_val_pred)))\n",
        "print(\"F1 score Val: {:.3}\".format(f1_score(y_val, y_val_pred, average='weighted')))\n",
        "print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy Train: 0.964\n",
            "F1 score Train: 0.962\n",
            "\n",
            "Accuracy Test: 0.971\n",
            "F1 score Test: 0.97\n",
            "\n",
            "Accuracy Val: 0.972\n",
            "F1 score Val: 0.971\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFGuFfKVo2e9"
      },
      "source": [
        "## Qualitative Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW7P84giGgP4"
      },
      "source": [
        "### **Question.**\n",
        "Is accuracy the metric that logistic regression optimizes while training? If no, which metric is optimized in logistic regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9NPftPBo2e9"
      },
      "source": [
        "## **Your answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14cDO5WWo2e9"
      },
      "source": [
        "Accuracy is not the direct metric that logistic regression optimizes when training. Logistic Regression minimizes i.e. optimizes $\\textbf{Negative Log Likelihood}$.\n",
        "\n",
        "$$min \\left(-\\sum_{i=1}^m (y^{i} \\times log(h_{\\theta} (x^{i})) + (1-y^{(i)}) \\times log(1 - h_{\\theta}(x^{(i)})))\\right) $$\n",
        "\n",
        "In training, attempts are made to find the best parameters for our logistic regression Model that minimizes the cost function. Hence, the minimized cost function induces higher accuracy metric. So in short, no, accuracy is not what model is optimizing while training. It is the cost function of negative log likelihood that is being optimized which in turn enhances or increases the accuracy. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak0h71krLPqX"
      },
      "source": [
        "### **Question.**\n",
        "In general, does having 0.99 accuracy on test means that the model is great? If no, can you give an example of a case when the accuracy is high but the model is not good? (Hint: why do we use F1 score?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brqkESI8o2e-"
      },
      "source": [
        "### **Your answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXlXyiqgo2e-"
      },
      "source": [
        "Having too high of accuracy such as 0.99 can be very misleading. Accuracy is the measure of True positive and True negative cases i.e. all the correct or true predictions made by the model. While this may be great for cases like Spam detector, accuracy may not be the best metric for cases where False positive and False Negative hold a lot of value. For example, for cancer detection, model accuracy may be 0.99 on 1000 test data point, but it does also mean that 10 people were misclassified by the model which can be very deterimental. Or taking modern example, imagine 10,000 people are in the test dataset that are being predicted for \"positive COVID.\" Our model's accuracy is 0.99. That means 100 people were misclassified as False negative or False positive. Due to 0.1 inaccuracy, COVID can spread to wider community. Thus, in this case, accuracy may not be the best metric or indicative of how good the model is. \n",
        "\n",
        "Due to this shortcoming of accuracy metric, F1 score hold significance as it is harmonic mean of precision and recall. F1 score penalizes or evaluates the model on False positives and False negatives. From the example above, F1 score will tell about the False negatives that were predicted by our model using the cancer patients data.  \n",
        "\n",
        "Thus, in short, accuracy is a good metric when True Positives and True Negatives hold significane. However, when False negatives and False positives are significant, F1-score will be a better metric than accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_RDI0qdOxwM"
      },
      "source": [
        "### Exploration of predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHR2OqYCDOxs"
      },
      "source": [
        "### **Question**\n",
        "\n",
        "Show a few examples with correctly predicted labels on the train and val sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf0zCTvyo2e-",
        "outputId": "fbb80b53-259b-404f-8222-defb9df2d14b"
      },
      "source": [
        "'''Training Data'''\n",
        "lst_Train_correct = []\n",
        "lst_Train_incorrect = []\n",
        "\n",
        "for tense, ori_label, pred_label in zip(train_texts, train_labels, y_train_pred):\n",
        "    if int(ori_label) == int(pred_label):\n",
        "        lst_Train_correct.append(tense)\n",
        "    else:\n",
        "        lst_Train_incorrect.append(tense)\n",
        "\n",
        "print(\"\\nTraining Data: Total Messages that were correctly classified: {} Misclassified: {}\".\\\n",
        "      format(len(lst_Train_correct),len(lst_Train_incorrect)))\n",
        "\n",
        "\n",
        "'''Change the index to see different classified sentences'''\n",
        "\n",
        "print(\"\\n\\033[1mCorrectly Classified Examples from Training Data Split:\\033[0m\\n\")\n",
        "for i in lst_Train_correct[:11]:\n",
        "    print(i)\n",
        "print()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Data: Total Messages that were correctly classified: 3758 Misclassified: 142\n",
            "\n",
            "\u001b[1mCorrectly Classified Examples from Training Data Split:\u001b[0m\n",
            "\n",
            "Convey my regards to him\n",
            "[Û_] anyway, many good evenings to u! s\n",
            "My sort code is  and acc no is . The bank is natwest. Can you reply to confirm i've sent this to the right person!\n",
            "Sorry i din lock my keypad.\n",
            "Ok i thk i got it. Then u wan me 2 come now or wat?\n",
            "Oi when you gonna ring\n",
            "Will be office around 4 pm. Now i am going hospital.\n",
            "Have you heard about that job? I'm going to that wildlife talk again tonight if u want2come. Its that2worzels and a wizzle or whatever it is?! \n",
            "Oh my God. I'm almost home\n",
            "No dear i do have free messages without any recharge. Hi hi hi\n",
            "Actually fuck that, just do whatever, do find an excuse to be in tampa at some point before january though\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yv8GD-UGXvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570bc894-bf00-4f04-f0c6-adbe8d576aee"
      },
      "source": [
        "lst_Val_correct = []\n",
        "lst_Val_incorrect = []\n",
        "\n",
        "for tense, ori_label, pred_label in zip(val_texts, val_labels, y_val_pred):\n",
        "    if int(ori_label) == int(pred_label):\n",
        "        lst_Val_correct.append(tense)\n",
        "    else:\n",
        "        lst_Val_incorrect.append(tense)\n",
        "\n",
        "print(\"\\nValidation Data: Total Messages that were correctly classified: {} Misclassified: {}\".\\\n",
        "      format(len(lst_Val_correct),len(lst_Val_incorrect)))\n",
        "\n",
        "\n",
        "'''Change the index to see different classified sentences'''\n",
        "\n",
        "print(\"\\n\\033[1mCorrectly Classified Examples from Validation Data Split:\\033[0m\\n\")\n",
        "for i in lst_Val_correct[:10]:\n",
        "    print(i)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Data: Total Messages that were correctly classified: 813 Misclassified: 23\n",
            "\n",
            "\u001b[1mCorrectly Classified Examples from Validation Data Split:\u001b[0m\n",
            "\n",
            "I just saw ron burgundy captaining a party boat so yeah\n",
            "I love you. You set my soul on fire. It is not just a spark. But it is a flame. A big rawring flame. XoXo\n",
            "Ok i vl..do u know i got adsense approved..\n",
            "HCL chennai requires FRESHERS for voice process.Excellent english needed.Salary upto  &lt;#&gt; .Call Ms.Suman  &lt;#&gt;  for Telephonic interview -via Indyarocks.com\n",
            "A boy was late 2 home. His father: \\POWER OF FRNDSHIP\\\"\"\n",
            "For ur chance to win å£250 cash every wk TXT: PLAY to 83370. T's&C's www.music-trivia.net custcare 08715705022, 1x150p/wk.\n",
            "Badrith is only for chennai:)i will surely pick for us:)no competition for him.\n",
            "Cool, we shall go and see, have to go to tip anyway. Are you at home, got something to drop in later? So lets go to town tonight! Maybe mum can take us in.\n",
            "Good afternoon starshine! How's my boytoy? Does he crave me yet? Ache to fuck me ? *sips cappuccino* I miss you babe *teasing kiss*\n",
            "Ya very nice. . .be ready on thursday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neMQ4VR9GVL3"
      },
      "source": [
        "### **Question** \n",
        "\n",
        "Print 10 examples from val set which were labeled incorrectly by the model. Why do you think the model got them wrong?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYklHyVgo2e-"
      },
      "source": [
        "### **Your answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ7fce8vo2e_"
      },
      "source": [
        "There can be different reasons for the misclassification. E.g. take the following example: \n",
        "                   \n",
        "                FreeMsg Hey U, i just got 1 of these video/pic fones, reply WILD to this txt & ill \n",
        "                send U my pics, hurry up Im so bored at work xxx (18 150p/rcvd STOP2stop)\n",
        "\n",
        "It was labeled as Spam, however, our model predicted it as Ham. It could be because most of the words in this message are regular tokens that are not being penalized. This message does not contain any phone number or any link to a site. Thus, it passes all of the additional features that were set in the Data Matrix. \n",
        "\n",
        "Or take this example: \n",
        "\n",
        "           Do you realize that in about 40 years, we'll have thousands of old ladies running around with tattoos?\n",
        "\n",
        "In the dataset, this has been labeled as Spam, however, our model predicted it as HAM. Along with the reasons mentioned for the previous examples, more or less, all the token words in this message are regular vocab phrases. Thus, the weightage of the message in comparison to the corpus is tilted towards HAM rather than SPAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-h7KUbeo2e_",
        "outputId": "3ff2478f-a9ae-4bdc-ae63-b8fe9e1e39cd"
      },
      "source": [
        "    \n",
        "print(\"\\n\\033[1m10 MisClassified Examples from Validation Data Split:\\033[0m\\n\")\n",
        "\n",
        "for i in lst_Val_incorrect[:11]:\n",
        "  print(i)\n",
        "    "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m10 MisClassified Examples from Validation Data Split:\u001b[0m\n",
            "\n",
            "WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
            "Hi, Mobile no.  &lt;#&gt;  has added you in their contact list on www.fullonsms.com It s a great place to send free sms to people For more visit fullonsms.com\n",
            "Latest News! Police station toilet stolen, cops have nothing to go on!\n",
            "This message is free. Welcome to the new & improved Sex & Dogging club! To unsubscribe from this service reply STOP. msgs@150p 18 only\n",
            "This message is free. Welcome to the new & improved Sex & Dogging club! To unsubscribe from this service reply STOP. msgs@150p 18+only\n",
            "Dorothy@kiefer.com (Bank of Granite issues Strong-Buy) EXPLOSIVE PICK FOR OUR MEMBERS *****UP OVER 300% *********** Nasdaq Symbol CDGT That is a $5.00 per..\n",
            "Do you realize that in about 40 years, we'll have thousands of old ladies running around with tattoos?\n",
            "Had your mobile 10 mths? Update to the latest Camera/Video phones for FREE. KEEP UR SAME NUMBER, Get extra free mins/texts. Text YES for a call\n",
            "WIN a å£200 Shopping spree every WEEK Starting NOW. 2 play text STORE to 88039. SkilGme. TsCs08714740323 1Winawk! age16 å£1.50perweeksub.\n",
            "Hi this is Amy, we will be sending you a free phone number in a couple of days, which will give you an access to all the adult parties...\n",
            "FreeMsg Hey U, i just got 1 of these video/pic fones, reply WILD to this txt & ill send U my pics, hurry up Im so bored at work xxx (18 150p/rcvd STOP2stop)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja1hoUIFp_C2"
      },
      "source": [
        "## End of Code\n",
        "\n",
        "# End of Notebook\n"
      ]
    }
  ]
}